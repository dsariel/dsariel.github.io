<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://dsariel.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://dsariel.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-02-14T09:54:30+00:00</updated><id>https://dsariel.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">HOWTO develop an OpenShift application</title><link href="https://dsariel.github.io/blog/2024/howto-develop-openshift-application/" rel="alternate" type="text/html" title="HOWTO develop an OpenShift application"/><published>2024-11-11T16:40:16+00:00</published><updated>2024-11-11T16:40:16+00:00</updated><id>https://dsariel.github.io/blog/2024/howto-develop-openshift-application</id><content type="html" xml:base="https://dsariel.github.io/blog/2024/howto-develop-openshift-application/"><![CDATA[<h1 id="howto-develop-an-openshift-application">HOWTO develop an OpenShift application</h1> <p>Since OpenShift’s container runtime poses constraints, one can’t simply create a <code class="language-plaintext highlighter-rouge">Containerfile</code> (or <code class="language-plaintext highlighter-rouge">Dockerfile</code>) and test a containerized application with Podman (or Docker), expecting it to be flawlessly deployed and work on OpenShift just because it worked with the Podman (or Docker) runtime.</p> <p>One possible approach (subject to limitations described below) is installing an all-in-one version of OpenShift and proceeding with the deployment, testing, and debugging cycles on the local machine.</p> <p>Using a local instance of OpenShift can save a considerable amount of time by eliminating the step of pushing large container images to a container registry. This allows you to create a container image locally and perform an OpenShift application deployment directly from that local image. However, this all-in-one version of OpenShift has limitations compared to a full-fledged OpenShift deployment across multiple nodes with dedicated hardware. For more details, see the <a href="https://docs.redhat.com/en/documentation/red_hat_codeready_containers/2.0/html/getting_started_guide/introducing-codeready-containers_gsg#differences-from-production-openshift-install_gsg">Red Hat CodeReady Containers documentation</a>.</p> <p>If your application requires features that are not supported by the all-in-one version, this method may not work for those applications (or may not test all the features of the application). Nevertheless, it can be useful for an initial kickstart and a subset of features.</p> <h2 id="how-to-install-red-hat-openshift-local-on-your-machine">How to install Red Hat OpenShift Local on your machine</h2> <p>With that said, follow the steps to deploy CRC locally as outlined in the <a href="https://docs.redhat.com/en/documentation/red_hat_codeready_containers/2.0/html/getting_started_guide/index">Red Hat OpenShift Local documentation</a>. The process of installation is very well explained just go throught it.</p> <h2 id="download">Download</h2> <p>A registration to <a href="https://console.redhat.com/openshift">Red Hat Hybrid Cloud console</a> is required in order to download CRC. Olthout there are several ways available I have choosen to authorize RedHat SSO with my github account</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"Red Hat SSO by rh-sso wants to access your github_user account" 
</code></pre></div></div> <p>and after filling a short form with details required to create a RedHat account, I was able to download the latest version of the OpenShift Local along with <code class="language-plaintext highlighter-rouge">pull secret</code>.</p> <h2 id="configuration-changes">Configuration changes</h2> <p>During the installation I have changed the CRC VM to take 12 VCPUs and 32 Gb of RAM even though less beaffy VM might also suite your application needs.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>crc config set cpus 12
crc config set memory 32768
</code></pre></div></div> <h2 id="troubleshooting">Troubleshooting</h2> <p>If something goes wrong with CRC instance simply delete the CRC VM, run cleanup to remove ~/.crc/machines and start over:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>crc delete
crc cleanup
crc setup
crc start 
</code></pre></div></div> <p>As mentioned at the begining Openshift containers have resringts. E.g. pods running on CoreOS are running as <code class="language-plaintext highlighter-rouge">root</code> and no writing is allowed as <code class="language-plaintext highlighter-rouge">root</code>.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
dev: could not create app data folder /.local/share/virtualenv due to PermissionError(13, 'Permission denied')
dev: find interpreter for spec PythonSpec(major=3, minor=12)
dev: proposed PythonInfo(spec=CPython3.12.0.final.0-64, exe=/usr/local/bin/python3.12, platform=linux, version='3.12.0 (main, Dec 9 2024, 01:35:29) [GCC 14.2.1 20240912 (Red Hat 14.2.1-3)]', encoding_fs_io=utf-8-utf-8)
usage: virtualenv [--version] [--with-traceback] [-v | -q] [--read-only-app-data] [--app-data APP_DATA] [--reset-app-data] [--upgrade-embed-wheels] [--discovery {builtin}] [-p py] [--try-first-with py_exe]
[--creator {builtin,cpython3-posix,venv}] [--seeder {app-data,pip}] [--no-seed] [--activators comma_sep_list] [--clear] [--no-vcs-ignore] [--system-site-packages] [--symlinks | --copies] [--no-download | --download]
[--extra-search-dir d [d ...]] [--pip version] [--setuptools version] [--wheel version] [--no-pip] [--no-setuptools] [--no-wheel] [--no-periodic-update] [--symlink-app-data] [--prompt prompt] [-h]
dest
virtualenv: error: argument dest: the destination . is not write-able at /app/.tox/dev
dev: FAIL code 2 (0.03 seconds)
evaluation failed :( (0.07 seconds)
</code></pre></div></div> <p>Coping your application to /tmp might work in some cases but not in their vast majority since application usually update folders in users’ HOMEDIR (e.g. local or .cache). So we will not consider this approach.</p> <p>A viable approach is to change the Containerfile to allow writing.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cat ~/.crc/machines/crc/kubeadmin-password 
or
crc console --credentials


oc login -u kubeadmin https://api.crc.testing:6443

oc get route default-route -n openshift-image-registry --template=''

podman login -u developer -p $(oc whoami -t) default-route-openshift-image-registry.apps-crc.testing

podman tag images.paas.redhat.com/atropos/eodweb default-route-openshift-image-registry.apps-crc.testing/demo/my-image:latest

podman push default-route-openshift-image-registry.apps-crc.testing/demo/my-image:latest
</code></pre></div></div> <hr/> <p>Another probem one might have is</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> "0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/disk-pressure: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling."
</code></pre></div></div> <p>To see what’s going on the CRC VM ssh command is</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ssh core@$(sudo virsh domifaddr crc | awk '/ipv4/ {print $4}' | cut -d'/' -f1) -i ~/.crc/machines/crc/id_ed25519
</code></pre></div></div> <p>if it does not looks ok, restart CRC configuring more space</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>crc config set disk-size 100
</code></pre></div></div> <h2 id="puch-an-image-form">puch an image form</h2> <h2 id="understanding-security-context-constraints-sccs-in-openshift">Understanding Security Context Constraints (SCCs) in OpenShift</h2> <p>In OpenShift, Security Context Constraints (SCCs) play a crucial role in controlling the permissions of containers and maintaining the security of the cluster. To grasp the significance of SCCs, it’s essential to understand how containers interact with protected Linux functions.</p> <h2 id="what-is-a-security-context-constraint">What Is a Security Context Constraint?</h2> <p>When a container runs a process, it inherently restricts that process from accessing certain privileged Linux functionalities. These limitations include actions such as:</p> <ul> <li>Accessing shared file systems</li> <li>Running as a privileged container or as root</li> <li>Executing specific Linux commands, such as <code class="language-plaintext highlighter-rouge">kill</code> These restrictions are in place to maintain container isolation. If processes within containers had unrestricted access, they could interfere with other processes and potentially compromise the isolation and security of the system.</li> </ul> <p>However, there are situations where applications require access to some of these protected functions. This is where Security Contexts and Security Context Constraints come into play.</p> <h2 id="how-security-contexts-work">How Security Contexts Work</h2> <p>A Security Context defines the security privileges and access controls applied to a pod or container. When an application needs access to certain protected Linux functions, the pod running the container must be configured with a security context that specifies the required access.</p> <p>Key components of the configuration include:</p> <ul> <li>The pod’s security context which details the access permissions needed.</li> <li>A service account associated with the pod that authorizes these permissions.</li> </ul> <h2 id="the-scc-authorization-process">The SCC Authorization Process</h2> <p>The process for granting these permissions involves several steps:</p> <ol> <li><strong>Developer creates an application</strong>: The developer builds an application that requires access to certain protected functions.</li> <li><strong>Deployer writes a deployment manifest</strong>: This manifest specifies the security context and the service account needed for the application.</li> <li><strong>Association with an SCC</strong>: The deployment is linked to an SCC, either predefined by OpenShift or custom-created by the cluster administrator.</li> <li><strong>Cluster administrator control</strong>: The SCC is created and managed by the cluster administrator, who has full control over which pods can access which functionalities.</li> </ol> <h2 id="how-pods-are-authorized">How Pods Are Authorized</h2> <p>When the deployment manifest is applied:</p> <ul> <li>The pod specifies the service account to be used.</li> <li>The service account is associated with an SCC, directly or through RBAC (Role-Based Access Control).</li> <li>Admission control checks if the pod’s security context aligns with the permissions defined in the SCC. If the request is valid, the pod is allowed to run with the specified access; otherwise, it is denied.</li> </ul> <h2 id="example-pod-manifest-and-deployment">Example: Pod Manifest and Deployment</h2> <p>In a typical deployment manifest, security contexts can be set at two levels:</p> <ul> <li><strong>Container-level security context</strong>: Specifies access permissions for an individual container.</li> <li><strong>Pod-level security context</strong>: Applies permissions to all containers within the pod.</li> </ul> <p>The pod manifest also includes the service account name used to authorize the specified access.</p> <h2 id="structure-of-security-context-constraints">Structure of Security Context Constraints</h2> <p>SCCs can either be predefined or custom-made by cluster administrators. Here’s a comparison between a restricted SCC and a custom SCC:</p> <ul> <li><strong>Restricted SCC</strong>: This built-in SCC in OpenShift enforces strict limitations. It drops most capabilities and restricts user and group permissions, ensuring minimal access beyond default settings.</li> <li><strong>Custom SCC</strong>: Administrators can create tailored SCCs that provide specific permissions, such as allowing a user ID range (e.g., 1000-2000) or granting additional capabilities.</li> </ul> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">containers</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">resources</span><span class="pi">:</span> <span class="pi">{}</span>
    <span class="na">terminationMessagePath</span><span class="pi">:</span> <span class="s">/dev/termination-log</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">demo</span>
    <span class="na">command</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">tox</span>
      <span class="pi">-</span> <span class="s">-v</span>
      <span class="pi">-</span> <span class="s">-e dev</span>
    <span class="na">securityContext</span><span class="pi">:</span>
      <span class="na">capabilities</span><span class="pi">:</span>
        <span class="na">drop</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="s">ALL</span>
      <span class="na">runAsUser</span><span class="pi">:</span> <span class="m">1234</span>
      <span class="na">runAsGroup</span><span class="pi">:</span> <span class="m">5678</span>
      <span class="na">runAsNonRoot</span><span class="pi">:</span> <span class="kc">true</span>
      <span class="na">allowPrivilegeEscalation</span><span class="pi">:</span> <span class="kc">false</span>
</code></pre></div></div> <p>Let me know if you need any additional formatting or edits!</p> <h2 id="conclusion">Conclusion</h2> <p>SCCs are essential for maintaining a secure and controlled environment in OpenShift. While containers by default limit access to protect system integrity, SCCs provide the flexibility needed when applications require elevated permissions. This balance ensures that cluster administrators can grant necessary permissions while maintaining overall security.</p>]]></content><author><name></name></author><category term="OpenShift"/><category term="OpenShift"/><category term="crc"/><category term="application"/><category term="development"/><category term="CodeReadyContainers"/><summary type="html"><![CDATA[OpenShift container runtime poses restrictions, and simple application containerization will not work in the vast majority of cases. In this post, steps are described to download CRC locally, containerize an application, apply necessary adjustments to overcome the restrictions, and push the resulting tested image to a registry, from which a production OpenShift instance will pull it.]]></summary></entry><entry><title type="html">HOWTO Obtain new Openshift token from Xterm</title><link href="https://dsariel.github.io/blog/2024/howto-obtain-new-openshift-token-from-xterm/" rel="alternate" type="text/html" title="HOWTO Obtain new Openshift token from Xterm"/><published>2024-02-14T09:15:16+00:00</published><updated>2024-02-14T09:15:16+00:00</updated><id>https://dsariel.github.io/blog/2024/howto-obtain-new-openshift-token-from-xterm</id><content type="html" xml:base="https://dsariel.github.io/blog/2024/howto-obtain-new-openshift-token-from-xterm/"><![CDATA[<h1 id="how-to-obtain-a-new-token-for-openshift-from-xterm">How to Obtain a New Token for OpenShift from xterm</h1> <p>Suppose your OpenShift login token expired because tokens are, by default, valid for a limited time (e.g., 24 hours). Follow these steps to obtain a new token and continue working with the cluster:</p> <h2 id="steps-to-obtain-a-new-token">Steps to Obtain a New Token</h2> <ol> <li> <p><strong>Install a Text-Based Browser</strong><br/> Install <code class="language-plaintext highlighter-rouge">lynx</code>, a command-line browser, to access the token request page:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>yum <span class="nb">install </span>lynx <span class="nt">-y</span>
</code></pre></div> </div> </li> <li> <p><strong>Access the Token Request Page</strong><br/> Use <code class="language-plaintext highlighter-rouge">lynx</code> to navigate to the token request page:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>lynx https://oauth-openshift.apps.ocp.openstack.lab/oauth/token/request
</code></pre></div> </div> </li> <li> <p><strong>Retrieve the **</strong><strong>``</strong><strong>** Password</strong><br/> If you are using the default <code class="language-plaintext highlighter-rouge">kubeadmin</code> user, retrieve the password from the installation directory:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cat</span> ~/.kube/kubeadmin-password
</code></pre></div> </div> <p>Use this password to log in as <code class="language-plaintext highlighter-rouge">kubeadmin</code> in the browser interface.</p> </li> <li> <p><strong>Generate a New Token</strong><br/> Once logged in, obtain a new token. The login command will look like this:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>oc login <span class="nt">--token</span><span class="o">=</span>sha256~&lt;...&gt; <span class="nt">--server</span><span class="o">=</span>https://api.ocp.openstack.lab:6443
</code></pre></div> </div> </li> <li> <p><strong>Retry Your Command</strong><br/> With the new token, re-run the desired command. For example:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>oc <span class="nt">-n</span> openstack <span class="nb">wait </span>openstackcontrolplane controlplane <span class="nt">--for</span> <span class="nv">condition</span><span class="o">=</span>Ready <span class="nt">--timeout</span><span class="o">=</span>60m
</code></pre></div> </div> </li> </ol> <p>By following these steps, you can quickly regain access to your OpenShift cluster and continue your tasks without disruption.</p>]]></content><author><name></name></author><category term="OpenShift"/><category term="OpenShift"/><category term="token"/><category term="xterm"/><category term="text-web-browser"/><category term="lynx"/><summary type="html"><![CDATA[OpenShift login token expired because tokens are, by default, valid for a limited time (e.g., 24 hours). Follow these steps to obtain a new token and continue working with the cluster.]]></summary></entry></feed>